quantizers:
  wrpn_quantizer:
    class: WRPNQuantizer
    bits_activations: 8
    bits_weights: 8
    bits_overrides:
    # Don't quantize first and last layer
      conv1: #conv1
        wts: 4
        acts: 4
      layers.0.conv1: #Depthwise conv
        wts: 4
        acts: 4
      layers.0.conv2: #Pointwise conv
        wts: 8
        acts: 8
      layers.1.conv1: #Depthwise conv
        wts: 4
        acts: 4
      layers.1.conv2: #Pointwise conv
        wts: 8
        acts: 8
      layers.2.conv1: #Depthwise conv
        wts: 4
        acts: 4
      layers.2.conv2: #Pointwise conv
        wts: 4
        acts: 4
      layers.3.conv1: #Depthwise conv
        wts: 4
        acts: 4
      layers.3.conv2: #Pointwise conv
        wts: 8
        acts: 8
      layers.4.conv1: #Depthwise conv
        wts: 4
        acts: 4
      layers.4.conv2: #Pointwise conv
        wts: 8
        acts: 8
      layers.5.conv1: #Depthwise conv
        wts: 4
        acts: 4
      layers.5.conv2: #Pointwise conv
        wts: 8
        acts: 8
      layers.6.conv1: #Depthwise conv
        wts: 4
        acts: 4
      layers.6.conv2: #Pointwise conv
         wts: 8
         acts: 8
      layers.7.conv1: #Depthwise conv
        wts: 4
        acts: 4
      layers.7.conv2: #Pointwise conv
        wts: 8
        acts: 8
      layers.8.conv1: #Depthwise conv
        wts: 4
        acts: 4
      layers.8.conv2: #Pointwise conv
        wts: 8
        acts: 8
      layers.9.conv1: #Depthwise conv
        wts: 4
        acts: 4 
      layers.9.conv2: #Pointwise conv
        wts: 8
        acts: 8
      layers.10.conv1: #Depthwise conv
        wts: 4
        acts: 4
      layers.10.conv2: #Pointwise conv
        wts: 8
        acts: 8
      layers.11.conv1: #Depthwise conv
        wts: 4
        acts: 4
      layers.11.conv2: #Pointwise conv
        wts: 8
        acts: 8
      layers.12.conv1: #Depthwise conv
        wts: 4
        acts: 4
      layers.12.conv2: #Pointwise conv
        wts: 8
        acts: 8
      linear: #fc_softmax
        wts: 4
        acts: 4

lr_schedulers:
  training_lr:
    class: StepLR
    step_size: 1
    gamma: 0.1
    
policies:  
    - quantizer:
        instance_name: wrpn_quantizer
      starting_epoch: 0
      ending_epoch: 400
      frequency: 1
    - lr_scheduler:
        instance_name: training_lr
      starting_epoch: 0
      ending_epoch: 170
      frequency: 1