quantizers:
  wrpn_quantizer:
    class: WRPNQuantizer
    bits_activations: 8
    bits_weights: 8
    bits_overrides:
    # Don't quantize first and last layer
      conv1: #conv1
         wts: 8
         acts: 8
      # layer1.0.conv1:
         # wts: 8
         # acts: 8
      # layer1.0.conv2:
         # wts: 8
         # acts: 8
      # layer1.1.conv1:
         # wts: 8
         # acts: 8
      # layer1.1.conv2:
         # wts: 8
         # acts: 8
      # layer1.2.conv1:
         # wts: 8
         # acts: 8
      # layer1.2.conv2:
         # wts: 8
         # acts: 8
      # layer2.0.conv1:
         # wts: 8
         # acts: 8
      # layer2.0.conv2:
         # wts: 8
         # acts: 8
      # layer2.0.downsample.0:
         # wts: 8
         # acts: 8
      # layer2.1.conv1:
         # wts: 8
         # acts: 8
      # layer2.1.conv2:
         # wts: 8
         # acts: 8
      # layer2.2.conv1:
         # wts: 8
         # acts: 8
      # layer2.2.conv2:
         # wts: 8
         # acts: 8
      # layer2.3.conv1:
         # wts: 8
         # acts: 8
      # layer2.3.conv2:
         # wts: 8
         # acts: 8
      # layer3.0.conv1:
         # wts: 8
         # acts: 8
      # layer3.0.conv2:
         # wts: 8
         # acts: 8
      # layer3.0.downsample.0:
         # wts: 8
         # acts: 8
      # layer3.1.conv1:
         # wts: 8
         # acts: 8
      # layer3.1.conv2:
         # wts: 8
         # acts: 8
      # layer3.2.conv1:
         # wts: 8
         # acts: 8
      # layer3.2.conv2:
         # wts: 8
         # acts: 8
      # layer3.3.conv1:
         # wts: 8
         # acts: 8
      # layer3.3.conv2:
         # wts: 8
         # acts: 8
      # layer3.4.conv1:
         # wts: 8
         # acts: 8
      # layer3.4.conv2:
         # wts: 8
         # acts: 8
      # layer3.5.conv1:
         # wts: 8
         # acts: 8
      # layer3.5.conv2:
         # wts: 8
         # acts: 8
      # layer4.0.conv1:
         # wts: 8
         # acts: 8
      # layer4.0.conv2:
         # wts: 8
         # acts: 8
      # layer4.0.downsample.0:
         # wts: 8
         # acts: 8
      # layer4.1.conv1:
         # wts: 8
         # acts: 8
      # layer4.1.conv2:
         # wts: 8
         # acts: 8
      # layer4.2.conv1:
         # wts: 8
         # acts: 8
      # layer4.2.conv2:
         # wts: 8
         # acts: 8
      fc: #fc_softmax
         wts: 8
         acts: 8

lr_schedulers:
  training_lr1:
    class: StepLR
    step_size: 2
    gamma: 0.89
  training_lr2:
    class: MultiStepLR
    milestones: [10, 20, 30, 35]
    gamma: 0.1
    
policies:  
    - quantizer:
        instance_name: wrpn_quantizer
      starting_epoch: 0
      ending_epoch: 200
      frequency: 1
    - lr_scheduler:
        instance_name: training_lr2
      starting_epoch: 0
      ending_epoch: 141
      frequency: 1